<!DOCTYPE HTML>
<html>
	<head>
		<title>Pioneer - Feature Flags</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="shortcut icon" href="./images/pioneer_branding/png/color/favicon.ico" type="image/x-icon" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Sidebar -->
			<section id="sidebar">
				<div class="inner">
					<nav>
						<ul>
							<li><a href="#intro">Pioneer</a></li>
							<li><a href="#one">1. Introduction and Use Case</a></li>
							<li><a href="#two">2. Potential Solutions</a></li>
							<li><a href="#three">3. What is Pioneer?</a></li>
							<li><a href="#four">4. Architectural Decisions</a></li>
							<li><a href="#five">5. Future Work</a></li>
							<li><a href="#six">6. References</a></li>
							<li><a href="#seven">7. Presentation</a></li>
							<li><a href="#eight">8. Meet the Team</a></li>
						</ul>
					</nav>
				</div>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">
				<a id="octocat" href="https://github.com/pioneer-io">
					<img src="./images/GitHub-Mark/PNG/GitHub-Mark-64px.png" alt="">
				</a>
				<!-- Intro -->
					<section id="intro" class="wrapper style1 fullscreen fade-up">
							<img id="logo-banner" src="./images/pioneer_branding/png/color/pioneer_color logo.png" alt="">
						<div id="logo-intro" class="inner">
							<p>Introducing the <em>fastest</em>, <em>easiest</em> way to move to microservices with simple, scalable feature flags.</p>
							<p>Available as an <strong>open-source</strong> project under the <a href="https://opensource.org/licenses/MIT">Open Source Initiative.</a></p>
							<ul class="actions">
								<li><a href="#one" class="button scrolly">Read the writeup</a></li>
							</ul>
						</div>

					</section>


				<!-- One -->
					<section id="one" class="wrapper style1 spotlights">
						<section>
							<div class="content">
								<div class="inner">
									<h2>1. Introduction & Use Case</h2>
									<p>Pioneer is a feature management service built to handle an organization’s migration from a monolithic architecture to a microservices architecture.</p>
									<p>As an application grows and demands on the system increase, an organization may find that they need to begin scaling their application. A monolithic architecture may naturally lead to tightly coupled code that is difficult to scale. Conversely, a microservices architecture is more loosely coupled and can be independently scaled & deployed. A small team can also organize a microservices architecture around business capabilities.</p>
									<p>When transitioning from a monolith to a microservices-oriented architecture, an organization may initially want to expose the new service to a small number of users while it collects analytics and user feedback, and analyze how the service performs under various loads. By using feature flags with Pioneer to handle this transition, any change can be quickly rolled back in real-time simply by toggling the flag off; no redeployment is necessary.</p>
								</div>
								<div id="one.one" class="inner">
									<h3>1.1 Hypothetical</h3>
									<p>To better understand this use case, we can consider a hypothetical situation involving a <i>company</i>, <b>Harvest Delivery</b>; we can use this hypothetical scenario to illustrate what the catalysts for a conversion from a monolith to microservices might be for a small team, to consider the challenges that this organization will be facing before undertaking such a project, and to examine some of the potential options that are available in this space.</p>
									<p>Harvest Delivery is a regional shopping service that allows users to order groceries online. A locally-contracted shopper purchases the requested groceries and delivers them to the doorstep of the user. Harvest Delivery’s web application receives a variable amount of user traffic, with peak traffic occurring in the days leading up to major holidays.</p>
									<img class="image fit" src="./images/harvest_delivery/harvest_delivery_landing.png" alt="the landing page of harvest delivery">
									<p>The architecture of Harvest Delivery’s web application is currently a monolith. As the organization grows, the monolithic architecture begins to cause a strain on the engineering team. One such strain is that the team has become reluctant to add new features to the monolithic codebase. Currently, different components of the code are highly dependent on one another and changing one component means an engineer also has to update multiple other components, increasing their workload. This issue also makes the codebase difficult to maintain, as fixing one bug has the tendency to introduce new problems.</p>
									<p>Another issue the team has encountered is the inability to scale each business component  (like payment processors or catalog) of their architecture independently. The team has found that during periods of high traffic, users are experiencing long delays during the payment stage. Harvest Delivery would like to scale the payment processing component independently from the rest of the codebase, but this isn’t possible with the current monolith. If the code responsible for payments was abstracted into a microservice, not only would this enable independent scaling, it would also cultivate a team solely dedicated to this payment service. The payment team could carry out their development and deployment pipeline independently to the rest of the monolith, resulting in faster improvements to payment processing.</p>
									<p>The final issue the Harvest Delivery team is encountering with their monolith pertains to availability. Currently, if a bug triggers an outage, then the entire application becomes unavailable. The team would like to ensure that even if one component of the application is unavailable, for example creating a new user account, users not using the affected component can still perform their desired actions.</p>
									<p>In response to these issues, the CTO has decided that they should migrate the application code towards a microservices architecture. The CTO has instructed the team to develop a system architecture organized around Harvest Delivery’s business concerns, with the shopping catalog, payment processing, and shopper communication all abstracted into individual microservices.</p>
									<p>Modifying the system architecture of Harvest Delivery is a significant undertaking; additionally, it’s important to avoid any impact on user experience. The holidays are coming up and Harvest Delivery does not want to lose any customers to system outages.</p>
									<p>Harvest Delivery plans to collect analytics and perform load testing on the new services to ensure that they can handle the load of holiday shopping sprees. They also plan to solicit user feedback from a percentage of users before rolling these new services out to the entire customer base.</p>
								</div>
							</div>
						</section>
					</section>
			
				<!-- Two -->
					<section id="two" class="wrapper style1 spotlights">
						<section>
							<div class="content">
								<div class="inner">
									<h2>2. Potential Solutions</h2>
									<p>The Harvest Delivery team needs to develop a strategy that enables them to achieve two primary goals - migrate to a microservices architecture and avoid system outages during the migration. This section discusses the available options to achieve these goals.</p>
									<h3 id="two.one">2.1 Canary Deployment</h3>
									<p>Harvest Delivery could consider a canary deployment, also known as side-by-side deployment, which involves creating a clone of the application’s production environment. A load balancer is used to initially send all traffic to one version while new functionality is built in the other version (the canary). When the new service is deployed, some percentage of users are directed to the canary deployment. If no issues arise, the service can be gradually rolled out to more users until the new version is used by everyone. If there are problems, deployments can be rolled back, and the majority of users will not be impacted <sup><a href="http://www.split.io/glossary/canary-deployment/">1</a></sup>.</p>
									<p>While this is a potential solution for Harvest Delivery, there are significant drawbacks to a canary deployment. Canary deployments result in an additional layer of complexity. The engineering team would need to deal with multiple production environments, monitor an additional system, and migrate users <sup><a href="http://www.split.io/glossary/canary-deployment/">1</a></sup>. A canary deployment would also require Harvest Delivery to maintain additional infrastructure.</p>
									<p>Canary deployments operate at the deployment level; therefore, if an issue arises the most recent deployment will need to be rolled back entirely. Unexpected deployment rollbacks can result in additional downtime, degrading user experience for those users who were previously routed to the canary deployment. Additionally, the engineers responsible for incident management must be able to fully respond to incidents immediately, in order to prevent problems from having a significant impact. Such incidents are likely to result in lost revenue and a damaged reputation for the company.</p>
									<p>Another drawback related to canary deployments is that engineers still lack the granular control to develop features in parallel and roll them out to users independently of one another. A user is either routed to the canary deployment or the original deployment. Therefore, only one percentage rollout can dictate the routing of users. For example, imagine that one feature or service within the canary is ready for a 70% rollout, but another is only ready to be rolled out to 5% of users. This discrepancy limits us to directing only 5% of users to the canary deployment. If there is a significant problem with a feature and we need to make sure that no users are exposed to that feature, we must either roll back a deployment or route 0% of users to the canary deployment until the problem has been fixed. Again, this magnifies the impact of each issue that the engineering team encounters during the transition from a monolith to a microservices architecture.</p>
									<p>The above limitations are problematic for the CTO of Harvest Delivery who wants to migrate to using several microservices over a period of time. The team needs the ability to roll out or roll back each microservice independently. Therefore, canary deployment isn’t suitable for Harvest Delivery due to their requirement for a solution that provides granular control over each individual microservice and its rollout status.</p>
								</div>
								<div class="inner">
									<h3 id="two.two">2.2 Feature Flags</h3>
									<p>The requirement for granular control over individual microservices has led Harvest Delivery to consider feature flags. Feature flags allow for one feature to be rolled out or rolled back completely independently of another, without a hotfix or redeployment <sup><a href="http://www.martinfowler.com/bliki/FeatureToggle.html/">5</a></sup>. Feature flags work by incorporating conditional branching logic into the application code and evaluating the boolean status of a feature flag. For example, if Harvest Delivery wishes to test a new payment processing microservice then they need to add a conditional statement to the monolith, at the point where the current monolithic payment processing code is invoked. The conditional statement will return a boolean value indicating whether the flag is toggled “on” (true) or “off” (false). If the feature flag is toggled on, then a call to the microservice is executed and the subsequent response is handled. If the feature flag is off, the original monolithic code is executed.</p>
									<p>In addition to enabling independent control over the rollout status of each microservice, feature flags also eliminate the need for frequent redeployment as seen with a canary deployment approach. This is because whilst a canary deployment lives in the infrastructure networking layer, feature flags live within the application and are evaluated in real time <sup><a href="http://www.martinfowler.com/bliki/FeatureToggle.html/">5</a></sup>. When rollout-related incidents occur and feature flags are employed, a microservice can be toggled off immediately and the original monolith code can be executed, rather than waiting for a potentially time-consuming redeployment. This makes incident management easier and provides the engineering team the time required to track down a bug and develop a robust solution to the cause of the incident. Moreover, disruption to the user experience is minimized, preventing an outage resulting in a loss of revenue.</p>
									<p>The ability to “switch off” new features in response to an issue means feature flags substantially mitigate the risk of engineering teams releasing immature functionality <sup><a href="http://www.featureflags.io/feature-flags/">3</a></sup>. This low-risk experimentation allows small teams to maximize developer efficiency and release new functionality with confidence, knowing that they have the option of quickly reversing course without affecting the rest of the application.</p>
									<p>Feature flags also enable microservices to be rolled out to a certain percentage of users. This occurs by using a unique identifier for each individual user, such as an IP address or user ID. The identifier is used by a hashing algorithm in the feature flag logic, which determines if that individual user falls within the current percentage rollout strategy (i.e. if a feature flag is being rolled out to 10% of users, does this individual user fall within that 10%?). Features that are toggled “off” will never be served, regardless of rollout percentage. If a feature flag is toggled “on”, the hashing algorithm of the  Software Development Kit (SDK) will determine whether the user’s unique identifier falls within the rollout percentage. If so, the flag will evaluate as `true`, and the feature will be served to the user.</p>
									<p>In conclusion, feature flags meet the requirement of the Harvest Delivery team to have control over the rollout status of each microservice independently. If issues arise from a new microservice, then feature flags enable the microservice to be “switched off” in real-time, without the need for redeployment. This allows Harvest Delivery to minimize any user disruption during the architectural changes, which reduces revenue losses and reputation impact. Moreover, Harvest Delivery can rollout microservices to a specified percentage of users, enabling analytics on the new service to be collected.</p>
								</div>
								<div class="inner">
									<h3 id="two.three">2.3 Feature Flags as a Solution</h3>
									<p>Harvest Delivery have decided to move forward with using feature flags to assist in their migration from a monolith to a microservices architecture. This section outlines how Harvest Delivery could integrate feature flags into their existing system.</p>
									<div class="inner">
										<h4 id="two.three.one">2.3.1 Developing a Feature Flag Service In-house</h4>
										<p>Harvest Delivery could choose to develop their own feature flag service. One naive approach would be to maintain a simple feature flag rule set within their application code as a configuration file. However, this would require re-deployment any time the team wished to toggle a feature on or off, or update the rollout percentage. Ultimately, this would negate the benefit of using feature flags.</p>
										<p>A more robust solution would be for the Harvest Delivery engineering team to build a true feature flag management system that would allow flags to be updated and dynamically evaluated without re-deploying any code. However, this would require engineering hours to build, test, and maintain the service. Harvest Delivery is a small team without excess engineering resources to devote to such a project. Additionally, the CTO prefers to move quickly into the transition to microservices, rather than waiting for an additional tool to be developed. A third-party solution that will work out-of-the-box is a better fit for the small, fast-moving team.</p>
									</div>
									<div class="inner">
										<h4 id="two.three.two">2.3.2 Existing Third-Party Solutions</h4>
										<p>Harvest Delivery requires an easy-to-use solution for their fast-moving team. The existing third-party solutions are evaluated below.</p>
										<ul class="alt">
											<li>
												<b>Launch Darkly</b>
												<p>LaunchDarkly is an enterprise-level feature management service. It’s a feature-rich, hosted service that includes a wide variety of features. However, because it is proprietary software, it comes at a monetary cost. For Harvest Delivery, LaunchDarkly’s plethora of features is beyond their needs.</p>
											</li>
											<li>
												<b>Cloud Bees</b>
												<p>CloudBees is another hosted service with proprietary software. It provides end-to-end DevOps features, which may be a great fit for larger teams with a focus on DevOps. Unfortunately, Harvest Delivery is a small team and is not concerned with the wide variety of DevOps tools CloudBees provides.</p>
											</li>
											<li>
												<b>FeatureHub</b>
												<p>FeatureHub is an open-source, self-hosted service. Because it is open-source, it can be customized to fit the unique needs of the team. FeatureHub focuses on feature flags for both client-side and server-side features. It offers an array of different flag types, along with complex logic allowing for organizations, users, and a variety of feature sets; the robust features and permissions management make it less accessible for a small team like Harvest Delivery.</p>
											</li>
											<li>
												<h4 id="two.three.three">2.3.3 Thoughts on Existing Third-Party Solutions</h4>
												<p>Enterprise solutions have a large array of financially costly DevOps services that may strain a small, regional organization like Harvest Delivery. Existing open-source solutions are complicated by focusing on client-side features evaluated in the browser and a litany of complex features including user permissions and service accounts. A small business like Harvest Delivery doesn’t need the extra expenses of an enterprise service or the unnecessary complexity that other offerings entail.</p>
											</li>
										</ul>
									</div>
								</div>
							</div>
						</section>
					</section>
			
				<!-- Three -->
					<section id="three" class="wrapper style1 spotlights">
						<section>
							<div class="content">
								<div class="inner">
									<h2>3. Introducing Pioneer</h2>
									<p>The limitations of the third-party solutions has led the Harvest Delivery team to pursue a new feature flag management system that fits their requirements perfectly: Pioneer.</p>
									<h3 id="three.one">3.1 What Is Pioneer?</h3>
									<p>Pioneer is an open-source, self-hosted feature flag management service that aids in the transition from a monolithic to microservices architecture. Feature flags can be used to rollout new services to all users, or an assigned percentage of users, and can easily be toggled on/off with a single click. These feature flags, and any updates made to them, are propagated to the client application in real-time, in an asynchronous and fault-tolerant manner.</p>
								</div>
								<div class="inner">
									<h3 id="three.two">3.2 Revisiting the Problem</h3>
									<p>Pioneer is a lightweight software that will allow Harvest Delivery to test and migrate their new microservices in a production environment under increasing load, without having to make any additional changes to their infrastructure, such as cloning the production environment.</p>
									<p>Using Pioneer to aid in the transition from a monolith to microservices architecture will reduce risk by allowing for immediate rollback without requiring a re-deployment or any additional downtime for the application. Services can be developed in parallel and rolled out at an independent rate because toggling a flag on or off, or changing its rollout percentage, does not impact any other functionality in the application. This will allow the small engineering team at Harvest Delivery to experiment in an agile manner with confidence.</p>
									<p>Pioneer is specifically built to support the evaluation of flags by returning a boolean value based on either their toggle status or their associated rollout percentage. As every flag will ultimately return a boolean value, they are an excellent fit for the use case in which requests should be routed in one direction or the other, either to a new external microservice or an existing feature internal to the monolith.</p>
								</div>
								<div class="inner">
									<h3 id="three.three">3.3 Using Pioneer</h3>
									<p>Pioneer has a simple feature set, which minimizes the set-up costs associated with configuring a new tool. This enables Pioneer users to quickly start a migration towards a microservices architecture. Below, we outline how Pioneer can be used to get started with this migration.</p>
									<p>Users that wish to use Pioneer out-of-the-box can simply clone the <a href="https://github.com/pioneer-io/pioneer">Pioneer GitHub repository</a> and start up the application with a single command, <code>docker-compose up</code>. This functionality is possible because we provide a <code>docker-compose.yml</code> and <code>.env</code> file that will configure and launch Pioneer in a Docker network.</p>
									<img class="image fit" src="./images/docker/docker_network.png" alt="all components in a docker network with cute whales">
									<p>The Pioneer application is composed of several components: Compass, Scout, and NATS JetStream. Compass is the primary application and offers a graphical user interface (GUI) built on React, as well as an API and Postgres database on the backend. Compass communicates directly with a NATS JetStream server, which relays messages to the Scout daemon. Scout communicates with all connected SDK clients in a unidirectional manner to provide up-to-date feature flag data.</p>
									<p>Feature flag data lives in the Compass application. Users can create, read, update, or delete flags via the Compass UI or the Compass API. Compass also provides the user with an SDK key, which is required for SDK client authorization (discussed in <a href="four.four">section 4.4</a>).</p>
									<p>Feature flags are evaluated in the user’s codebase via our server-side SDKs. Pioneer currently offers SDKs written in Node.js, Ruby, and Golang. Once an SDK has been installed, the user must provide the aforementioned SDK key in order to successfully receive data. On application startup, authorized SDKs will connect to the Scout daemon as a server-sent events (SSE) client. All SDK clients connected to Scout will receive the feature flag data, and any subsequent changes to it, in real-time.</p>
									<p>A Pioneer user utilizes the feature flag data by incorporating conditional branching logic into their application code and evaluating the boolean status of a feature flag. For example, if a user wishes to migrate to a new microservice for payment processing, they could create a new flag on the Compass GUI called <code>payment_processor</code> and toggle the flag to “on”. Within their application code they would add an <code>if/else</code> statement, which evaluates the status of the <code>payment_processor</code> flag and executes the appropriate code. When the <code>payment_processor</code> flag is toggled on, evaluating the flag will return <code>true</code> and the code responsible for managing the payment processing via calling the new microservice will be executed. If the Pioneer user decides not to use the microservice anymore, the <code>payment_processor</code> flag can be toggled “off” on the Compass GUI. The next time <code>payment_processor</code> is evaluated by the SDK, it will return <code>false</code> and the monolith code will be executed.</p>
									<img class="image fit" src="./images/code_blocks/microservice_if_else.png" alt="if else conditional code block">
									<p>Pioneer offers an accessible, real-time feature flag service integrated with a user’s codebase and getting started takes just minutes. Because Pioneer is completely open-source, organizations with specific needs that fall outside of the default configuration can add their own customization to make Pioneer fit their own requirements.</p>
								</div>
								<div class="inner">
									<h3 id="three.four">3.4 Where Pioneer Fits</h3>
									<p>When thinking about third-party feature flagging solutions, it is important to consider them with certain criteria. We believe that when comparing existing options, there are four pivotal criteria to consider.</p>
									<ul class="alt">
										<li>The first is <b>flexibility</b>. A flexible solution will allow users to have greater control over the product itself. Open-source solutions are more flexible because they allow an organization to modify the software to suit their specific needs. Proprietary software does not provide this same flexibility.</li>
										<li>The next is <b>accessibility</b>. Solutions with high accessibility are easy to set up and easy to use. Less-accessible products may require user account setups, complex permissions, and dense documentation that a user must parse through in order to achieve the appropriate configuration.</li>
										<li><b>Affordability</b> is also an important criteria. Small and mid-sized teams may not have the fiscal resources needed to comfortably afford third-party solutions with a significant monetary cost.  Low-cost or no-cost options are more affordable, but may lack features and support. Higher-cost options mean paying for the services, but may also come with a more robust feature set and highly-available support services.</li>
										<li>The last criteria to consider is <b>simplicity</b>. Simpler solutions will have a less robust feature set, but instead focus on a core set of features tailored to a specific use case. Because proprietary software does not allow users to customize the software themselves, they may offer a larger feature set in order to fit a wider variety of use cases. However, this can result in bloated products that have many features the user does not require. For this reason, we consider simplicity to be a benefit, not a drawback.</li>
										<li>
											<img class="image fit" src="./images/pioneer_visuals/pioneer_comp_chart.png" alt="chart showing pioneer is flexible, affordable, and accessible, but doesn't offer a rich featureset">
											<p>With these criteria in mind, we can see that Pioneer is the only option that is <b>flexible</b>, <b>accessible</b>, and <b>affordable</b>. It does not provide a robust feature set, but <em>this simplicity makes it the best choice</em> for organizations who want a solution that they can use to manage their feature flags straight away with no extra work.</p>
										</li>
									</ul>
								</div>
							</div>
						</section>
	
				<!-- Four -->
					<section id="four" class="wrapper style1 spotlights">
						<section>
							<div class="content">
								<div class="inner">
									<h2>4. Techinical Deep Dive</h2>
									<p>Pioneer consists of three main components: Compass, NATS JetStream and Scout, all of which are run within a Docker network. Additionally, there is an SDK embedded in the user’s application.</p>
									<img class="image fit" src="./images/pioneer_visuals/pioneer_architecture.png" alt="diagram showing pioneer's architecture; there are 4 components: Compass, NATS, Scout, and SDKs">
									<p>Below we will discuss each component’s role in the Pioneer architecture. For more information regarding the engineering decisions that led to this architecture, refer to <a href=”five.five”>section 5</a>.</p>
								</div>
								<div class="inner">
									<h3>4.1 Hosted vs. Self-Hosted</h3>
									<p>One of the first decisions we discussed was whether to provide pioneer as a hosted or self-hosted application.  ultimately, we value allowing pioneer’s users to fully customize the software as needed to their own particular needs.  distributing pioneer as an open-source, self-hosted product allows users to tweak pioneer to fit their needs perfectly.</p>
									<p>Additionally, because pioneer’s users will be hosting their own instance of pioneer, they do not have to be concerned with the security of a multi-tenancy architecture hosted by an external organization.  pioneer’s users maintain full control of their own data.</p>
								</div>
								<div class="inner">
									<h3>4.2 How to Provide Flag Updates</h3>
									<p>We spent a great deal of effort determining the best way to provide feature flag updates to the Pioneer SDK installed in the user’s application.  Some options we considered included: API polling, webhooks, websockets, and streaming.</p>
									<p>While API polling seemed to be the simplest approach, it would require waiting for an SDK client to poll the Compass API for an update. This would eliminate the ability to provide feature flag updates in real-time, and may also result in unnecessary network traffic.</p>
									<p>Webhooks was another alternative the Pioneer team considered. Webhooks generally provide faster updates than polling while being more efficient because of their event driven nature. However, it would require an additional HTTP endpoint to be exposed on the client application. Pioneer strives not to interfere with the client application.</p>
									<p>Websockets are a good fit for bi-directional communication. Although the SDK client initially sends a ruleset request to Scout to initialize a connection, all subsequent messages are sent from Scout to the SDK. Therefore, for the most part, Scout only requires unidirectional communication to the SDK client. We don’t need the SDK client to communicate in a back-and-forth fashion regarding feature flag data. Instead, we simply want to push these feature flag updates down to the SDK clients.</p>
									<p>Ultimately we chose to go with streaming, in order to provide real-time updates from our server to the user application, rather than waiting for a client to poll the Compass API for an update. Continue below to read about how we chose the tools with which to stream updates.</p>
								</div>
								<div class="inner">
									<h3>4.3 Streaming Feature Flag Updates with JetStream</h3>
									<p>We chose to stream feature flag data from Compass to the Scout daemon. Our streaming tool of choice, NATS JetStream, allows for decoupled messaging. Naively, we could have required the Scout daemon to communicate directly with the Compass API. However, Compass would then have the additional responsibility of tracking all listening Scout instances and ensuring message delivery. Using a streaming tool to handle message delivery allows for a better separation of concerns, and allows Compass to worry simply about publishing the correct message.  We chose not to pursue Kafka because its complexity and larger infrastructure were unnecessary for our use case. NATS has a smaller infrastructure and provides all of the features needed for Pioneer’s use case.</p>
									<p>NATS streaming allows for many-to-one communication. This means that as an organization scales, for example, they could choose to also horizontally scale the number of Scout daemons sending updates to SDK clients.  Any Scout daemons subscribed to the NATS stream would receive feature flag updates as usual. Alternatively, a logging service could also subscribe to the NATS stream and preserve messages for later analysis.</p>
									<p>Additionally, we wanted to use a streaming tool that allowed for guaranteed delivery of messages so that missed updates would not be a concern. This is why we specifically chose to use NATS JetStream over its predecessor, Core NATS. JetStream allows for guaranteed message delivery. This alleviates concerns of missed messages resulting in incorrect feature flag data.  NATS JetStream will send the most up-to-date ruleset to all subscribers. In our case, Scout is the subscriber. The message will wait until the subscriber receives and acknowledges it.  This means that if the Scout daemon goes offline, it will receive the most up-to-date feature flag data as soon as it comes back online.  This guaranteed message delivery means that we do not have to worry about missed messages due to an unreliable network. In short, JetStream will store the messages in memory. For every message that arrives, it will check the message sequence number to ensure that the send is caught up on messages. If not, JetStream will resend the data to the inbox of the sending node.</p>
								</div>
								<div class="inner">
									<h3>4.4 Server Sent Events and SDK Keys</h3>
									<p>Server-sent events provide a unidirectional mode of communication between the Scout daemon and SDK clients.  Again, although API polling was another option by which SDK clients could have requested feature flag updates from Scout, our preference for real-time updates eliminated API polling as a possible solution.</p>
									<p>We chose to send a server-sent event from Scout to all connected SSE clients, which are Pioneer’s SDKs that are downloaded on the server-side of the user’s application. Upon receiving a new server-sent event, the SDK will parse the newly provided data and use the updated ruleset to evaluate feature flags.</p>
									<p>One additional concern the Pioneer team discussed was how to authorize SSE clients.  We decided that when a client attempted to connect to Scout and receive feature flag data, we should first determine whether the client should be able to access that data.  This is done by providing an SDK key for usage by SDKs downloaded into the organization’s codebase.  The SDKs then send the provided SDK key with each request to connect as an SSE client to Scout. If no valid key is provided, the request to connect will be rejected.</p>
								</div>
								<div class="inner">
									<h3>4.5 Redis Cache</h3>
									<p>Initially, we considered whether we would need to use a Redis cache to offload read requests of Compass’ PostgreSQL database triggered by Scout requesting an updated feature flag ruleset.  The proposed cache would request data from the Compass API in order to populate its initial cache, and listen for subsequent ruleset updates.</p>
									<p>However, we determined after further analysis that adding a cache to our architecture was not appropriate for our use case and would unnecessarily increase the complexity of Pioneer’s architecture.  Because the use case for Pioneer is relevant to small or medium-sized organizations, the number of read operations on Compass’ PostgreSQL database is entirely manageable without a cache.</p>
								</div>
								<div class="inner">
									<h3>4.6 Ruleset Transmission Formatting</h3>
									<p>Another tough decision that the Pioneer team had to make surrounded the content of messages that were distributed throughout the system. Namely, if our use case demanded that information about specific feature flags was being passed down to the SDKs, should we opt to send only the information pertaining to the specific feature that was changed, or send the ruleset in its entirety, on every transmission?</p>
									<p>In the end, we chose to implement Pioneer such that, regardless of operation, be it a request for a ruleset or a distribution of an update to the SDKs, the system would send the whole ruleset.</p>
									<p>As we see it, this method of distribution offers a handful of advantages. The first of which is that by transmitting the full ruleset, we can ensure that every SDK will store the most up to date rule set available. Imagine if we only transferred update information about a single flag at a time. Suppose then, that an SDK misses an update to a given feature flag due to networking issues. The next update that comes through might be for the same flag, in which case there might be only small repercussions. However, suppose then that the next update that is received is for a different flag, and that the flag in question is not updated again for several hours. This would mean that that SDK could contain outdated feature flag data that could possibly persist for quite a while. By sending the full ruleset, any time there is a change of any kind, the SDK will receive a full, updated ruleset. In this way, we can significantly reduce the potential for SDKs serving outdated feature flag data.</p>
									<p>An additional benefit to sending the whole ruleset down is that it allows the code on either end to be simple and elegant. Because we work with the whole ruleset, we don’t have to worry about introducing additional surface area for bugs by including logic for breaking apart rulesets, updating specific elements, and handling every type of CRUD operation that might occur.</p>
									<p>The obvious tradeoff that came to mind was the increased size of messages that would result from sending whole rulesets, and the impact on network bandwidth that this might cause. While it is true that the size of messages would increase with whole rulesets, we tried to frame this in the context of our use case to put the potential impact in a realistic spotlight. As Pioneer is designed to be used with small teams for the purposes of migrating services from a monolith, we reasoned that the standard use case would not likely exceed 20-30 distinct flags at a time. But, just to be safe we tested with a ruleset composed of 100 distinct flags. Even at this seemingly overinflated ruleset size, the total size of transmission (from Scout to SDKs, including headers) was almost exactly 20KB. WIth an expected rate of 10 requests/second, we felt the impact of 2MB/second should fall well within the tolerances of any modern network</p>
									<p>So, with that in mind, we concluded that the tradeoff of increased transmission size for sending full rulesets was acceptable when weighed against the benefits added to the system by their inclusion.</p>
								</div>
								<div class="inner">
									<h3>4.7 Flag Types Provided</h3>
									<p>Our team considered the value of adding additional types of flags, similar to other existing feature flag services. Some of the many types of flags offered by other feature flag services include boolean, number, string, and JSON.</p>
									<p>Ultimately, boolean flags are by far the most relevant flag type for Pioneer’s use case of strangling a monolith.  These flags are evaluated on the server side of the application, and are only concerned with whether a service is toggled on or off.</p>
									<p>Adding additional types of flags may offer more flexibility for users of Pioneer, but it would result in a higher degree of complexity for Pioneer’s design as well as the user’s interactions with Compass.  Ultimately, we decided that adding additional flag types did not line up with the priorities of the Pioneer team at this time.</p>
								</div>
								<div class="inner">
									<h3>4.8 Load Testing</h3>
									<p>One area with which we wanted to take extra consideration was understanding and testing the limitations of how many SDKs can simultaneously connect to Scout and be served rulesets efficiently. To do this we wanted to define our expected use case, so that we might better set explicit performance metrics.</p>
									<p>Our logic was as follows: Pioneer supports feature flag services for one back-end application at a time, which means the upper bounds of simultaneous SDK connections to the Scout daemon will depend on the extent to which that application has been horizontally scaled (ie: how many instances of that application are running at the same time).</p>
									<p>Our expectation is that the small to medium sized dev team that is working with Pioneer will have an application that typically has no more than 100 concurrent instances of that application running at one time. We estimate that Scout would conservatively have around 10 new SDK connections being made every second requesting new rulesets.</p>
									<p>WIth this use case in mind we reasoned that a rate of 10 new connections to Scout every second would cover most usage scenarios that could reasonably be expected. But also bearing in mind that unpredictable peak usage periods might occur, we also reasoned that 100 requests per second could be a realistic possibility during a peak usage period.</p>
									<p>Again, in our tests with Scout, we wanted to simulate the process of an SDK client establishing an SSE connection with Scout and receiving the full ruleset data payload. The tested ruleset included 100 different flags, with a total object size of 18.7KB after extraction from Postgres and serialization into JSON. With this ruleset size in mind, including HTTP headers, the response from scout to SDK is almost 20KB exactly.</p>
									<p>Because we wanted to simulate the process of connecting and fetching an initial ruleset only, we made a small modification to the Scout daemon that did not leave SSE connections open, but rather, closed them after the flag data had been retrieved and sent to the SDK. If each connection from every SDK staying open indefinitely the performance tolerances of the Scout daemon would most certainly perform differently, however, we felt that because SDK connections are likely to be closed and added with regularity as the client application spins up new instances and terminates others, it is reasonable to test scout without leaving every connection open in perpetuity.</p>
									<p>Our testing procedure utilized Artillery with a configuration that ran several different stages for extended periods of time, with levels of traffic increasing tenfold from 1 request per second all the way up to 1000 requests per second before ramping back down.</p>
									<p>The results of these proved a few things. Firstly, the Scout daemon can easily handle the expected case of 10 requests per second. Second, peak usage load of 100 requests per second showed some small increases to latency, but the system still served the data payloads without any drops. Lastly, while performance degraded under 1000 requests per second, the tests show that Scout ( combined with JetStream & Compass ) are more than capable of handling the anticipated amount of load from one application, horizontally scaled with around 10 new instances per second.</p>
								</div>
								<div class="inner">
									<h3>4.9 NoSQL or SQL</h3>
									<p>Compass uses Postgresql as its associated database. We initially chose MongoDb because of its high scalability features, ease of horizontal scalability, and very fast reads and writes. Since we were not using a cache, this would reduce the chance of the Compass database becoming a system wide bottleneck. We used NoSQL to facilitate a rapid development process where we would not have to get bogged down with ERDs and the possibility of having to update schema/migrate data if there was a change to our data's proposed structure as we moved through stages of development. Furthermore, since the bulk of our data would be stored in rulesets of nested javascript objects, there would be lower impedance between the in memory representation of the ruleset and the database document model representation.</p>
									<p>However, we realized that for our use case, there would not be as many reads as needed for a MongoDb level speed of reading. In addition, there would be far fewer writes, as flag creation would only happen a few times, and toggling would happen at most only a few times a day. The schema complexity would not be an issue, since there were only a few entities that needed to be created.</p>
									<p>Maturity and stability. Relational databases still have the edge here in maturity and stability. People are familiar with how they work, what they can do, and have confidence in their reliability. There are also more programmers and toolsets available for relational databases. So when in doubt, this is the road that will be traveled. For our use case, companies like Harvest involved in the shopping business are more likely to rely on SQL for their data integrity properties. Since part of our advantage was being open sourced for the engineering team to tweak to their satisfaction, using a SQL database would be more natural and familiar for them.</p>
									<p>In addition, we realized that our queries would be fairly simple enough so that the impedance between JSON and SQL would be minimal. Furthermore, since our database is part of the monolith, communicating on the same machine with the Compass app server, there was no need for the horizontal scaling capabilities or fault tolerant clustering properties of MongoDb, which would be used in a more distributed environment. We decided that in sum, using a SQL database would not be a source of system wide bottlenecks for use cases akin to Harvest company.</p>
								</div>
								<div class="inner">
									<h3>4.9 NoSQL or SQL</h3>
									<p>Compass uses Postgresql as its associated database. We initially chose MongoDb because of its high scalability features, ease of horizontal scalability, and very fast reads and writes. Since we were not using a cache, this would reduce the chance of the Compass database becoming a system wide bottleneck. We used NoSQL to facilitate a rapid development process where we would not have to get bogged down with ERDs and the possibility of having to update schema/migrate data if there was a change to our data's proposed structure as we moved through stages of development. Furthermore, since the bulk of our data would be stored in rulesets of nested javascript objects, there would be lower impedance between the in memory representation of the ruleset and the database document model representation.</p>
									<p>However, we realized that for our use case, there would not be as many reads as needed for a MongoDb level speed of reading. In addition, there would be far fewer writes, as flag creation would only happen a few times, and toggling would happen at most only a few times a day. The schema complexity would not be an issue, since there were only a few entities that needed to be created.</p>
									<p>Maturity and stability. Relational databases still have the edge here in maturity and stability. People are familiar with how they work, what they can do, and have confidence in their reliability. There are also more programmers and toolsets available for relational databases. So when in doubt, this is the road that will be traveled. For our use case, companies like Harvest involved in the shopping business are more likely to rely on SQL for their data integrity properties. Since part of our advantage was being open sourced for the engineering team to tweak to their satisfaction, using a SQL database would be more natural and familiar for them.</p>
									<p>In addition, we realized that our queries would be fairly simple enough so that the impedance between JSON and SQL would be minimal. Furthermore, since our database is part of the monolith, communicating on the same machine with the Compass app server, there was no need for the horizontal scaling capabilities or fault tolerant clustering properties of MongoDb, which would be used in a more distributed environment. We decided that in sum, using a SQL database would not be a source of system wide bottlenecks for use cases akin to Harvest company.</p>
								</div>
								<div class="inner">
									<h3>4.10 Scaling</h3>
									<p>Pioneer is set up to deploy a single instance of the Scout daemon, but users can scale horizontally as needed by deploying an additional instance of Scout on a separate port.  All instances of Scout will be subscribed to the appropriate NATS JetStream stream in order to receive feature flag updates, and will send these updates to all connected SDK clients via server-sent events.</p>
									<p>Additionally, if frequent read operations become too demanding on Compass’ PostgreSQL database, users may choose to expand the architecture of their self-hosted instance of Pioneer by adding a cache or a read-replica.</p>
									<p>Because Pioneer is intended for use by small to medium organizations, we don’t anticipate that this issue will apply to the vast majority of users.</p>
								</div>
							</div>
						</section>
					</section>				

				<!-- Five -->
					<section id="five" class="wrapper style1 spotlights">
						<section>
							<div class="content">
								<div class="inner">
									<h2>5. Future Work</h2>
									<h3>5.1 Accommodate Multiple Applications</h3>
									<p>Currently, an instance of Pioneer supports a single application. More specifically, Pioneer supports a single ruleset.  If an organization would like to use Pioneer with additional applications that require different rulesets, they will need to spin up an additional instance of Pioneer to communicate with that application. This is a natural consequence of Pioneer’s simplicity and ease-of-use. However, in the future we may consider adding support for multiple rulesets handled by a single instance of Pioneer.</p>
								</div>
								<div class="inner">
									<h3>5.2 Additional Rollout Strategies</h3>
									<p>Offering additional rollout strategies that allow the organization to target particular users would allow for more granular control over who the earliest users of a new service are.  Some special users that we may choose to accommodate in the future are internal users, a predetermined group of beta-testers, or particular segments of the market.</p>
								</div>
								<div class="inner">
									<h3>5.3 Flag Expiration</h3>
									<p>Because Pioneer is meant to be used to roll out new services, the feature flag for a service likely shouldn’t live in the codebase indefinitely. Flag expiration would allow engineers to set an expiration date on a flag after which the flag will throw an exception or log a warning message if it is evaluated in the codebase. The motivation behind flag expiration is to avoid technical debt. When a feature flag is no longer necessary, it should be removed from the codebase.</p>
								</div>
								<div class="inner">
									<h3>5.4 Multiple Environments for Single Applications</h3>
									<p>Supporting multiple environments such as testing and staging in addition to production may be a feature that we explore in the future. Different environments of the same application would require the same ruleset, however engineers may wish for a flag’s toggle status to be different in the testing environment versus production, for example. Accommodating multiple environments would add an additional layer of complexity, but it would offer more granular control to users of Pioneer.</p>
								</div>
								<div class="inner">
									<h3>5.5 Permission Management</h3>
									<p>Supporting multiple environments such as testing and staging in addition to production may be a feature that we explore in the future. Different environments of the same application would require the same ruleset, however engineers may wish for a flag’s toggle status to be different in the testing environment versus production, for example. Accommodating multiple environments would add an additional layer of complexity, but it would offer more granular control to users of Pioneer.</p>
								</div>
							</div>
						</section>
					</section>

		<!-- Six -->
					<section id="six" class="wrapper style1 spotlights">
						<section>
							<div class="content">
								<div class="inner">
									<h2>6. References</h2>
									<ol>
										<li><a href="https://martinfowler.com/bliki/CanaryRelease.html">Martin Fowler - Canary Release</a></li>
										<li><a href="https://www.split.io/glossary/canary-deployment/">Split.io - Canary Deployments</a></li>
										<li>ETC</li>
									</ol>
							</div>
						</section>
					</section>

		<!-- Seven -->
					<section id="seven" class="wrapper style1 spotlights">
						<section>
							<div class="content">
								<div class="inner">
									<h2>7. Presentation</h2>
									<div style="width:100%"><div style="height:0;padding-bottom:56.25%;position:relative;width:100%"><iframe allowfullscreen="" frameBorder="0" height="100%" src="https://giphy.com/embed/xU7TNYYXP2UWnYUqPC/video" style="left:0;position:absolute;top:0" width="100%"></iframe></div></div>

							</div>
						</section>
					</section>
	
		<!-- Eight -->
			<section id="eight" class="wrapper style1 spotlights">
				<section>
					<div class="content">
						<div class="inner">
							<h2>8. Meet the Team</h2>
							<p>Pioneer was built by a small team of dedicated individuals.</p>
							<p>We are currently looking for positions, so please reach out if this project interested you and we would love to chat more about it!</p>

						</div>
						<div class="box alt">
							<div class="row gtr-uniform">
								<div class="col-3 center">
									<img class="image fit" src="./images/team_photos/jimmy.png" alt="" />
									<p>Jimmy Zheng</p>
									<div class="row center">
										<a href="#">
											<img src="./images/email_icon-32.png" alt="">
										</a>
										<a href="#">
											<img src="./images/GitHub-Mark/PNG/GitHub-Mark-32px.png" alt="">
										</a>
										<a href="#">
											<img src="./images/LinkedIn-Logos/LI-In-Bug-BW-mini.png" alt="">
										</a>
									</div>
								</div>
								<div class="col-3 center">
									<img class="image fit" src="./images/team_photos/laura.jpg" alt="" />
									<p>Laura Davies</p>
									<div class="row center">
										<a href="#">
											<img src="./images/email_icon-32.png" alt="">
										</a>
										<a href="#">
											<img src="./images/GitHub-Mark/PNG/GitHub-Mark-32px.png" alt="">
										</a>
										<a href="#">
											<img src="./images/LinkedIn-Logos/LI-In-Bug-BW-mini.png" alt="">
										</a>
									</div>
								</div>
								<div class="col-3 center">
									<img class="image fit" src="./images/team_photos/kyle.jpg" alt="" />
									<p>Kyle Ledoux</p>
									<div class="row center">
										<a href="#">
											<img src="./images/email_icon-32.png" alt="">
										</a>
										<a href="#">
											<img src="./images/GitHub-Mark/PNG/GitHub-Mark-32px.png" alt="">
										</a>
										<a href="#">
											<img src="./images/LinkedIn-Logos/LI-In-Bug-BW-mini.png" alt="">
										</a>
									</div>
								</div>
								<div class="col-3 center">
									<img class="image fit" src="./images/team_photos/liz.png" alt="" />
									<p>Elizabeth Tackett</p>
									<div class="row center">
										<a href="#">
											<img src="./images/email_icon-32.png" alt="">
										</a>
										<a href="#">
											<img src="./images/GitHub-Mark/PNG/GitHub-Mark-32px.png" alt="">
										</a>
										<a href="#">
											<img src="./images/LinkedIn-Logos/LI-In-Bug-BW-mini.png" alt="">
										</a>
									</div>
								</div>
							</div>
						</div>
					</div>
				</section>
			</section>		
		</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper style1-alt">
				<div class="inner">
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>